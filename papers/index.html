<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Leif Hancox-Li">
    <meta name="description" content="Leif Hancox-Li&#39;s personal website">
    

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Papers"/>
<meta name="twitter:description" content="A list of my published papers and preprints in philosophy of machine learning, feminist philosophy, and philosophy of physics.
AI Ethics / Philosophy of Machine Learning &ldquo;Unsocial Intelligence: an Investigation of the Assumptions of AGI Discourse&rdquo;. Forthcoming in AIES 2024. Co-authored with Borhane Blili-Hamelin and Andrew Smart.
&ldquo;The Disagreement Problem in Faithfulness Metrics&rdquo;. In the NeurIPS 2023 XAI in Action workshop. Co-authored with Brian Barr, Noah Fatsi, Alden Richter, Caleb Mok, and Danny Proano."/>

    <meta property="og:title" content="Papers" />
<meta property="og:description" content="A list of my published papers and preprints in philosophy of machine learning, feminist philosophy, and philosophy of physics.
AI Ethics / Philosophy of Machine Learning &ldquo;Unsocial Intelligence: an Investigation of the Assumptions of AGI Discourse&rdquo;. Forthcoming in AIES 2024. Co-authored with Borhane Blili-Hamelin and Andrew Smart.
&ldquo;The Disagreement Problem in Faithfulness Metrics&rdquo;. In the NeurIPS 2023 XAI in Action workshop. Co-authored with Brian Barr, Noah Fatsi, Alden Richter, Caleb Mok, and Danny Proano." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://boltzmann-brain.github.io/papers/" /><meta property="article:section" content="" />
<meta property="article:published_time" content="2019-07-07T21:48:16-04:00" />
<meta property="article:modified_time" content="2019-07-07T21:48:16-04:00" />



    
      <base href="https://boltzmann-brain.github.io/papers/">
    
    <title>
  Papers · Leif Hancox-Li
</title>

    
      <link rel="canonical" href="https://boltzmann-brain.github.io/papers/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="https://boltzmann-brain.github.io/css/coder.min.28d751104f30c16da1aa1bb04015cbe662cacfe0d1b01af4f2240ad58580069c.css" integrity="sha256-KNdREE8wwW2hqhuwQBXL5mLKz&#43;DRsBr08iQK1YWABpw=" crossorigin="anonymous" media="screen" />
    

    

    

    

    
    
    <link rel="icon" type="image/png" href="https://boltzmann-brain.github.io/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://boltzmann-brain.github.io/images/favicon-16x16.png" sizes="16x16">

    <meta name="generator" content="Hugo 0.92.1" />
  </head>

  <body class=" ">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="https://boltzmann-brain.github.io/">
      Leif Hancox-Li
    </a>
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://boltzmann-brain.github.io/about/">About</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://boltzmann-brain.github.io/resume/">Experience</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://boltzmann-brain.github.io/writing/">Writings</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://boltzmann-brain.github.io/papers/">Papers</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://boltzmann-brain.github.io/posts/">Blog</a>
          </li>
        
      
      
    </ul>
  </section>
</nav>


      <div class="content">
        
  <section class="container page">
  <article>
    <header>
      <h1>Papers</h1>
    </header>

    <p>A list of my published papers and preprints in philosophy of machine learning, feminist philosophy, and philosophy of physics.</p>
<h2 id="ai-ethics--philosophy-of-machine-learning">AI Ethics / Philosophy of Machine Learning</h2>
<p>&ldquo;<a href="https://arxiv.org/abs/2401.13142">Unsocial Intelligence: an Investigation of the Assumptions of AGI Discourse</a>&rdquo;. Forthcoming in AIES 2024. Co-authored with Borhane Blili-Hamelin and Andrew Smart.</p>
<p>&ldquo;<a href="https://openreview.net/forum?id=KPtW2SU0my">The Disagreement Problem in Faithfulness Metrics</a>&rdquo;. In the NeurIPS 2023 <a href="https://xai-in-action.github.io">XAI in Action workshop</a>. Co-authored with Brian Barr, Noah Fatsi, Alden Richter, Caleb Mok, and Danny Proano.</p>
<ul>
<li>Post-hoc explainability methods for black-box machine learning models are sometimes evaluated based on how &ldquo;faithful&rdquo; they are to the black-box model. Our paper presents empirical evidence that various faithfulness metrics proposed by researchers actually disagree when they are used to &ldquo;rank&rdquo; different explanation methods.</li>
</ul>
<p>&ldquo;Making Intelligence: Ethical Values in IQ and ML Benchmarks&rdquo;. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT &lsquo;23). Co-authored with <a href="https://borhane.xyz">Borhane Blili Hamelin</a>.</p>
<ul>
<li>Uses lessons from feminist and anti-racist critiques of IQ benchmarks to point to similar risks in how we create machine learning benchmarks.</li>
<li><a href="https://dl.acm.org/doi/10.1145/3593013.3593996">Journal link</a>, <a href="https://arxiv.org/abs/2209.00692v4">Preprint</a></li>
</ul>
<p>&ldquo;Should attention be all we need? The epistemic and ethical implications of unification in machine learning.&rdquo; Forthcoming in Conference on Fairness, Accountability, and Transparency (FAccT &lsquo;22). Co-authored with Nic Fishman.</p>
<ul>
<li>Assesses the epistemic and ethical risks/benefits of the trend towards unified model architectures in machine learning.</li>
<li><a href="https://dl.acm.org/doi/abs/10.1145/3531146.3533206">Journal Link</a>, <a href="https://arxiv.org/abs/2205.08377">Preprint</a></li>
</ul>
<p>&ldquo;Epistemic values in feature importance methods: Lessons from feminist epistemology&rdquo;. Conference on Fairness, Accountability, and Transparency (FAccT &lsquo;21). Co-authored with Lizzie Kumar.</p>
<ul>
<li>Argues that many popular feature importance methods in ML adhere to a methodology and view of objectivity that is in tension with feminist epistemology.</li>
<li>Winner of FAccT 2021 best paper award.</li>
<li><a href="https://dl.acm.org/doi/10.1145/3442188.3445943">Journal link</a>, <a href="https://arxiv.org/abs/2101.12737">preprint</a></li>
</ul>
<p>&ldquo;<a href="https://ml-retrospectives.github.io/neurips2020/camera_ready/13.pdf">Beyond Methods Reproducibility in Machine Learning</a>&rdquo;.  <a href="https://ml-retrospectives.github.io/">ML-Retrospectives, Surveys &amp; Meta-Analyses Workshop</a> at NeurIPS 2020.</p>
<ul>
<li>Argues that reproducibility definitions in ML are too focused on methods reproducibility, when in fact we need different types of reproducibilities for different types of claims made by ML experiments.</li>
</ul>
<p>&ldquo;Robustness in Machine Learning Explanations: Does It Matter?&rdquo; Conference on Fairness, Accountability, and Transparency (FAccT ’20), January 27–30, 2020, Barcelona, Spain. ACM, New York, NY, USA.</p>
<ul>
<li>Applies the concept of derivational robustness to explanations generated for black-box machine learning models.</li>
<li><a href="https://dl.acm.org/doi/abs/10.1145/3351095.3372836">Journal link</a>, <a href="http://philsci-archive.pitt.edu/16734/">preprint</a></li>
</ul>
<h2 id="philosophy-of-physics">Philosophy of Physics</h2>
<p>&ldquo;Solutions in Constructive Field Theory,&rdquo; Philosophy of Science 84, no. 2 (April 2017): 335-358.</p>
<ul>
<li>On how the concept of a &ldquo;correct solution&rdquo; in mathematically rigorous quantum field theory depends on non-rigorous considerations.</li>
<li><a href="https://www.journals.uchicago.edu/doi/abs/10.1086/690722">Journal link</a>; <a href="http://philsci-archive.pitt.edu/12281/">preprint</a></li>
</ul>
<p>&ldquo;Coarse-Graining as a Route to Microscopic Physics: The Renormalization Group in Quantum Field Theory,&rdquo; Philosophy of Science 82, no. 5 (December 2015): 1211-1223.</p>
<ul>
<li>Why the renormalization group matters even if we care about removing all cutoffs from quantum field theory.</li>
<li>Winner of the Philosophy of Science Association&rsquo;s Recent PhD Essay Award.</li>
<li><a href="https://www.journals.uchicago.edu/doi/abs/10.1086/684085">Journal Link</a>, <a href="http://philsci-archive.pitt.edu/12268/">preprint</a></li>
</ul>
<p>&ldquo;Interpretive strategies for deductively insecure theories: The case of early quantum electrodynamics,&rdquo;
Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics,
Volume 44, Issue 4 (2013): 395-403.</p>
<ul>
<li>Historically interpreting early quantum electrodynamics through the lens of Bill Wimsatt&rsquo;s work on false models. Reading early QED in German was fun!</li>
<li><a href="https://www.sciencedirect.com/science/article/pii/S1355219813000701">Journal link</a>, <a href="https://www.dropbox.com/s/tvidvhtfddkcqaa/preprint-website.pdf?dl=0">preprint</a></li>
</ul>
<h2 id="feminist-philosophy">Feminist Philosophy</h2>
<p>&ldquo;Idealization and Abstraction in Models of Injustice&rdquo;, Hypatia, 32 (2017): 329-346.</p>
<ul>
<li>On whether feminist philosophers should eschew using idealized models in social and political theory.</li>
<li><a href="https://onlinelibrary.wiley.com/action/showCitFormats?doi=10.1111%2Fhypa.12317">Journal Link</a>, <a href="https://www.dropbox.com/s/yqlt8wsn9ko4bne/preprint-v4-ideal-theory.pdf?dl=0">preprint</a></li>
</ul>
<p>&ldquo;Defining the Terms of Our Struggle: On the Role of the Concept of Woman in Political Theory&rdquo;</p>
<ul>
<li>Yet another take on how we should define &ldquo;woman&rdquo; for the purposes of social justice. Attempts to be more trans-friendly than some other takes on the issue.</li>
<li><a href="https://www.dropbox.com/s/f3my0ti1jbvwk19/vspc_woman-project.pdf?dl=0">Unpublished manuscript</a>.</li>
</ul>

  </article>
</section>


      </div>

      <footer class="footer">
  <section class="container">
    
      <p>No, the code does not document itself.</p>
    
     © 2024
    
       · 
      Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
    
    
  </section>
</footer>

    </main>

    

  </body>

</html>
